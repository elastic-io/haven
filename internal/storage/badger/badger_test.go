package badger

import (
	"bytes"
	"fmt"
	"os"
	"strings" // æ·»åŠ ç¼ºå¤±çš„å¯¼å…¥
	"sync"
	"testing"
	"time"

	"github.com/dgraph-io/badger/v3" // æ·»åŠ ç¼ºå¤±çš„å¯¼å…¥
	"github.com/elastic-io/haven/internal/log"
	"github.com/elastic-io/haven/internal/storage"
	"github.com/elastic-io/haven/internal/types"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

// æµ‹è¯•è¾…åŠ©å‡½æ•°
func setupTestStorage(t *testing.T) (*BadgerS3Storage, func()) {
	log.Init("", "debug")
	// åˆ›å»ºä¸´æ—¶ç›®å½•
	tempDir, err := os.MkdirTemp("", "badger_test_")
	require.NoError(t, err)

	// åˆ›å»ºå­˜å‚¨å®ä¾‹
	storage, err := NewBadgerS3Storage(tempDir)
	require.NoError(t, err)

	badgerStorage := storage.(*BadgerS3Storage)

	// è¿”å›æ¸…ç†å‡½æ•°
	cleanup := func() {
		badgerStorage.Close()
		os.RemoveAll(tempDir)
	}

	return badgerStorage, cleanup
}

func createTestBucket(t *testing.T, s *BadgerS3Storage, bucketName string) {
	err := s.CreateBucket(bucketName)
	require.NoError(t, err)
}

func createTestObject(t *testing.T, s *BadgerS3Storage, bucket, key string, data []byte) {
	obj := &types.S3ObjectData{
		Key:          key,
		Data:         data,
		ContentType:  "application/octet-stream",
		LastModified: time.Now(),
		ETag:         storage.CalculateETag(data),
		Metadata:     make(map[string]string),
	}
	err := s.PutObject(bucket, obj)
	require.NoError(t, err)
}

// æµ‹è¯•NewBadgerS3Storage
func TestNewBadgerS3Storage(t *testing.T) {
	t.Run("æˆåŠŸåˆ›å»ºå­˜å‚¨å®ä¾‹", func(t *testing.T) {
		tempDir, err := os.MkdirTemp("", "badger_test_")
		require.NoError(t, err)
		defer os.RemoveAll(tempDir)

		storage, err := NewBadgerS3Storage(tempDir)
		assert.NoError(t, err)
		assert.NotNil(t, storage)

		badgerStorage := storage.(*BadgerS3Storage)
		assert.NotNil(t, badgerStorage.db)
		assert.NotNil(t, badgerStorage.cleanupDone)
		assert.NotNil(t, badgerStorage.cleanupTicker)

		badgerStorage.Close()
	})

	t.Run("æ— æ•ˆè·¯å¾„åº”è¯¥è¿”å›é”™è¯¯", func(t *testing.T) {
		// ä½¿ç”¨ä¸€ä¸ªä¸å­˜åœ¨çš„çˆ¶ç›®å½•
		invalidPath := "/nonexistent/path/to/badger"
		storage, err := NewBadgerS3Storage(invalidPath)
		assert.Error(t, err)
		assert.Nil(t, storage)
	})

	t.Run("æƒé™ä¸è¶³çš„è·¯å¾„åº”è¯¥è¿”å›é”™è¯¯", func(t *testing.T) {
		// åœ¨Unixç³»ç»Ÿä¸Šï¼Œå°è¯•åœ¨æ ¹ç›®å½•åˆ›å»ºæ•°æ®åº“
		if os.Getuid() != 0 { // érootç”¨æˆ·
			storage, err := NewBadgerS3Storage("/root/badger_test")
			assert.Error(t, err)
			assert.Nil(t, storage)
		}
	})
}

// æµ‹è¯•æ¡¶æ“ä½œ
func TestBucketOperations(t *testing.T) {
	s, cleanup := setupTestStorage(t)
	defer cleanup()

	t.Run("åˆ›å»ºæ¡¶", func(t *testing.T) {
		err := s.CreateBucket("test-bucket")
		assert.NoError(t, err)

		// éªŒè¯æ¡¶å­˜åœ¨
		exists, err := s.BucketExists("test-bucket")
		assert.NoError(t, err)
		assert.True(t, exists)
	})

	t.Run("åˆ›å»ºé‡å¤æ¡¶åº”è¯¥è¿”å›é”™è¯¯", func(t *testing.T) {
		err := s.CreateBucket("duplicate-bucket")
		assert.NoError(t, err)

		err = s.CreateBucket("duplicate-bucket")
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "already exists")
	})

	t.Run("æ£€æŸ¥ä¸å­˜åœ¨çš„æ¡¶", func(t *testing.T) {
		exists, err := s.BucketExists("nonexistent-bucket")
		assert.NoError(t, err)
		assert.False(t, exists)
	})

	t.Run("åˆ—å‡ºæ¡¶", func(t *testing.T) {
		// åˆ›å»ºå¤šä¸ªæ¡¶
		buckets := []string{"bucket1", "bucket2", "bucket3"}
		for _, bucket := range buckets {
			err := s.CreateBucket(bucket)
			assert.NoError(t, err)
		}

		// åˆ—å‡ºæ¡¶
		result, err := s.ListBuckets()
		assert.NoError(t, err)
		assert.GreaterOrEqual(t, len(result), len(buckets))

		// éªŒè¯æ¡¶åç§°
		bucketNames := make([]string, len(result))
		for i, bucket := range result {
			bucketNames[i] = bucket.Name
		}

		for _, expectedBucket := range buckets {
			assert.Contains(t, bucketNames, expectedBucket)
		}
	})

	t.Run("åˆ é™¤ç©ºæ¡¶", func(t *testing.T) {
		bucketName := "empty-bucket"
		err := s.CreateBucket(bucketName)
		assert.NoError(t, err)

		err = s.DeleteBucket(bucketName)
		assert.NoError(t, err)

		// éªŒè¯æ¡¶ä¸å­˜åœ¨
		exists, err := s.BucketExists(bucketName)
		assert.NoError(t, err)
		assert.False(t, exists)
	})

	t.Run("åˆ é™¤éç©ºæ¡¶åº”è¯¥è¿”å›é”™è¯¯", func(t *testing.T) {
		bucketName := "non-empty-bucket"
		err := s.CreateBucket(bucketName)
		assert.NoError(t, err)

		// æ·»åŠ ä¸€ä¸ªå¯¹è±¡
		createTestObject(t, s, bucketName, "test-key", []byte("test-data"))

		err = s.DeleteBucket(bucketName)
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "not empty")
	})

	t.Run("åˆ é™¤ä¸å­˜åœ¨çš„æ¡¶åº”è¯¥è¿”å›é”™è¯¯", func(t *testing.T) {
		err := s.DeleteBucket("nonexistent-bucket")
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "not found")
	})

	t.Run("æ¡¶åç§°è¾¹ç•Œæµ‹è¯•", func(t *testing.T) {
		// æµ‹è¯•ç©ºæ¡¶å
		err := s.CreateBucket("")
		assert.NoError(t, err) // Badgerå…è®¸ç©ºé”®

		// æµ‹è¯•é•¿æ¡¶å
		longName := strings.Repeat("a", 1000)
		err = s.CreateBucket(longName)
		assert.NoError(t, err)

		// æµ‹è¯•ç‰¹æ®Šå­—ç¬¦
		specialName := "bucket-with-special-chars!@#$%^&*()"
		err = s.CreateBucket(specialName)
		assert.NoError(t, err)
	})
}

// æµ‹è¯•å¯¹è±¡æ“ä½œ
func TestObjectOperations(t *testing.T) {
	s, cleanup := setupTestStorage(t)
	defer cleanup()

	bucketName := "test-bucket"
	createTestBucket(t, s, bucketName)

	t.Run("å­˜å‚¨å’Œè·å–å¯¹è±¡", func(t *testing.T) {
		key := "test-object"
		data := []byte("Hello, World!")

		obj := &types.S3ObjectData{
			Key:          key,
			Data:         data,
			ContentType:  "text/plain",
			LastModified: time.Now(),
			ETag:         storage.CalculateETag(data),
			Metadata:     map[string]string{"author": "test"},
		}

		// å­˜å‚¨å¯¹è±¡
		err := s.PutObject(bucketName, obj)
		assert.NoError(t, err)

		// è·å–å¯¹è±¡
		retrieved, err := s.GetObject(bucketName, key)
		assert.NoError(t, err)
		assert.Equal(t, obj.Key, retrieved.Key)
		assert.Equal(t, obj.Data, retrieved.Data)
		assert.Equal(t, obj.ContentType, retrieved.ContentType)
		assert.Equal(t, obj.ETag, retrieved.ETag)
		assert.Equal(t, obj.Metadata, retrieved.Metadata)
	})

	t.Run("è·å–ä¸å­˜åœ¨çš„å¯¹è±¡åº”è¯¥è¿”å›é”™è¯¯", func(t *testing.T) {
		_, err := s.GetObject(bucketName, "nonexistent-key")
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "not found")
	})

	t.Run("ä»ä¸å­˜åœ¨çš„æ¡¶è·å–å¯¹è±¡", func(t *testing.T) {
		_, err := s.GetObject("nonexistent-bucket", "test-key")
		assert.Error(t, err)
	})

	t.Run("åˆ é™¤å¯¹è±¡", func(t *testing.T) {
		key := "object-to-delete"
		createTestObject(t, s, bucketName, key, []byte("data"))

		// éªŒè¯å¯¹è±¡å­˜åœ¨
		_, err := s.GetObject(bucketName, key)
		assert.NoError(t, err)

		// åˆ é™¤å¯¹è±¡
		err = s.DeleteObject(bucketName, key)
		assert.NoError(t, err)

		// éªŒè¯å¯¹è±¡ä¸å­˜åœ¨
		_, err = s.GetObject(bucketName, key)
		assert.Error(t, err)
	})

	t.Run("åˆ é™¤ä¸å­˜åœ¨çš„å¯¹è±¡åº”è¯¥è¿”å›é”™è¯¯", func(t *testing.T) {
		err := s.DeleteObject(bucketName, "nonexistent-key")
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "not found")
	})

	t.Run("åˆ—å‡ºå¯¹è±¡", func(t *testing.T) {
		// åˆ›å»ºå¤šä¸ªå¯¹è±¡
		objects := []string{"obj1", "obj2", "obj3", "folder/obj4", "folder/obj5"}
		for _, key := range objects {
			createTestObject(t, s, bucketName, key, []byte("data-"+key))
		}

		// åˆ—å‡ºæ‰€æœ‰å¯¹è±¡
		result, prefixes, err := s.ListObjects(bucketName, "", "", "", 100)
		assert.NoError(t, err)
		assert.GreaterOrEqual(t, len(result), len(objects))

		// éªŒè¯å¯¹è±¡é”®
		resultKeys := make([]string, len(result))
		for i, obj := range result {
			resultKeys[i] = obj.Key
		}

		for _, expectedKey := range objects {
			assert.Contains(t, resultKeys, expectedKey)
		}
		assert.Empty(t, prefixes) // æ²¡æœ‰åˆ†éš”ç¬¦æ—¶ä¸åº”è¯¥æœ‰å…¬å…±å‰ç¼€
	})

	t.Run("å¸¦å‰ç¼€åˆ—å‡ºå¯¹è±¡", func(t *testing.T) {
		// åˆ—å‡ºä»¥"folder/"å¼€å¤´çš„å¯¹è±¡
		result, _, err := s.ListObjects(bucketName, "folder/", "", "", 100)
		assert.NoError(t, err)

		for _, obj := range result {
			assert.True(t, strings.HasPrefix(obj.Key, "folder/"))
		}
	})

	t.Run("å¸¦åˆ†éš”ç¬¦åˆ—å‡ºå¯¹è±¡", func(t *testing.T) {
		// ä½¿ç”¨åˆ†éš”ç¬¦"/"
		_, prefixes, err := s.ListObjects(bucketName, "", "", "/", 100)
		assert.NoError(t, err)

		// åº”è¯¥æœ‰å…¬å…±å‰ç¼€"folder/"
		assert.Contains(t, prefixes, "folder/")
	})

	t.Run("åˆ†é¡µåˆ—å‡ºå¯¹è±¡", func(t *testing.T) {
		// é™åˆ¶è¿”å›æ•°é‡
		result, _, err := s.ListObjects(bucketName, "", "", "", 2)
		assert.NoError(t, err)
		assert.LessOrEqual(t, len(result), 2)
	})

	t.Run("å¯¹è±¡å¤§å°è¾¹ç•Œæµ‹è¯•", func(t *testing.T) {
		// æµ‹è¯•ç©ºå¯¹è±¡
		key := "empty-object"
		createTestObject(t, s, bucketName, key, []byte{})
		retrieved, err := s.GetObject(bucketName, key)
		assert.NoError(t, err)
		assert.Empty(t, retrieved.Data)

		// æµ‹è¯•å¤§å¯¹è±¡
		largeData := make([]byte, 1024*1024) // 1MB
		for i := range largeData {
			largeData[i] = byte(i % 256)
		}
		key = "large-object"
		createTestObject(t, s, bucketName, key, largeData)
		retrieved, err = s.GetObject(bucketName, key)
		assert.NoError(t, err)
		assert.Equal(t, largeData, retrieved.Data)
	})

	t.Run("å¯¹è±¡é”®è¾¹ç•Œæµ‹è¯•", func(t *testing.T) {
		// æµ‹è¯•ç©ºé”®
		createTestObject(t, s, bucketName, "", []byte("empty-key-data"))
		retrieved, err := s.GetObject(bucketName, "")
		assert.NoError(t, err)
		assert.Equal(t, []byte("empty-key-data"), retrieved.Data)

		// æµ‹è¯•é•¿é”®
		longKey := strings.Repeat("k", 1000)
		createTestObject(t, s, bucketName, longKey, []byte("long-key-data"))
		retrieved, err = s.GetObject(bucketName, longKey)
		assert.NoError(t, err)
		assert.Equal(t, []byte("long-key-data"), retrieved.Data)

		// æµ‹è¯•ç‰¹æ®Šå­—ç¬¦é”®
		specialKey := "key-with-special-chars!@#$%^&*()_+-=[]{}|;':\",./<>?"
		createTestObject(t, s, bucketName, specialKey, []byte("special-key-data"))
		retrieved, err = s.GetObject(bucketName, specialKey)
		assert.NoError(t, err)
		assert.Equal(t, []byte("special-key-data"), retrieved.Data)
	})
}

// æµ‹è¯•åˆ†æ®µä¸Šä¼ æ“ä½œ
func TestMultipartUploadOperations(t *testing.T) {
	s, cleanup := setupTestStorage(t)
	defer cleanup()

	bucketName := "test-bucket"
	createTestBucket(t, s, bucketName)

	t.Run("å®Œæ•´çš„åˆ†æ®µä¸Šä¼ æµç¨‹", func(t *testing.T) {
		key := "multipart-object"
		contentType := "application/octet-stream"
		metadata := map[string]string{"test": "value"}

		// 1. åˆå§‹åŒ–åˆ†æ®µä¸Šä¼ 
		uploadID, err := s.CreateMultipartUpload(bucketName, key, contentType, metadata)
		assert.NoError(t, err)
		assert.NotEmpty(t, uploadID)

		// 2. ä¸Šä¼ åˆ†æ®µ
		parts := []types.MultipartPart{}
		partData := [][]byte{
			[]byte("part1-data"),
			[]byte("part2-data"),
			[]byte("part3-data"),
		}

		for i, data := range partData {
			partNumber := i + 1
			etag, err := s.UploadPart(bucketName, key, uploadID, partNumber, data)
			assert.NoError(t, err)
			assert.NotEmpty(t, etag)

			parts = append(parts, types.MultipartPart{
				PartNumber: partNumber,
				ETag:       etag,
			})
		}

		// 3. åˆ—å‡ºåˆ†æ®µ
		listedParts, err := s.ListParts(bucketName, key, uploadID)
		assert.NoError(t, err)
		assert.Len(t, listedParts, len(parts))

		// éªŒè¯åˆ†æ®µä¿¡æ¯
		for i, part := range listedParts {
			assert.Equal(t, i+1, part.PartNumber)
			assert.Equal(t, len(partData[i]), part.Size)
			assert.NotEmpty(t, part.ETag)
		}

		// 4. å®Œæˆåˆ†æ®µä¸Šä¼ 
		finalETag, err := s.CompleteMultipartUpload(bucketName, key, uploadID, parts)
		assert.NoError(t, err)
		assert.NotEmpty(t, finalETag)

		// 5. éªŒè¯æœ€ç»ˆå¯¹è±¡
		obj, err := s.GetObject(bucketName, key)
		assert.NoError(t, err)

		expectedData := bytes.Join(partData, nil)
		assert.Equal(t, expectedData, obj.Data)
		assert.Equal(t, contentType, obj.ContentType)
		assert.Equal(t, metadata, obj.Metadata)
	})

	t.Run("ä¸­æ­¢åˆ†æ®µä¸Šä¼ ", func(t *testing.T) {
		key := "aborted-multipart"

		// åˆå§‹åŒ–åˆ†æ®µä¸Šä¼ 
		uploadID, err := s.CreateMultipartUpload(bucketName, key, "text/plain", nil)
		assert.NoError(t, err)

		// ä¸Šä¼ ä¸€ä¸ªåˆ†æ®µ
		_, err = s.UploadPart(bucketName, key, uploadID, 1, []byte("test-data"))
		assert.NoError(t, err)

		// ä¸­æ­¢ä¸Šä¼ 
		err = s.AbortMultipartUpload(bucketName, key, uploadID)
		assert.NoError(t, err)

		// éªŒè¯åˆ†æ®µä¸å­˜åœ¨
		_, err = s.ListParts(bucketName, key, uploadID)
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "not found")
	})

	t.Run("åˆ—å‡ºåˆ†æ®µä¸Šä¼ ", func(t *testing.T) {
		// åˆ›å»ºå¤šä¸ªåˆ†æ®µä¸Šä¼ 
		uploads := []string{}
		for i := 0; i < 3; i++ {
			key := fmt.Sprintf("multipart-key-%d", i)
			uploadID, err := s.CreateMultipartUpload(bucketName, key, "text/plain", nil)
			assert.NoError(t, err)
			uploads = append(uploads, uploadID)
		}

		// åˆ—å‡ºåˆ†æ®µä¸Šä¼ 
		result, err := s.ListMultipartUploads(bucketName)
		assert.NoError(t, err)
		assert.GreaterOrEqual(t, len(result), len(uploads))

		// éªŒè¯ä¸Šä¼ ID
		resultIDs := make([]string, len(result))
		for i, upload := range result {
			resultIDs[i] = upload.UploadID
		}

		for _, expectedID := range uploads {
			assert.Contains(t, resultIDs, expectedID)
		}
	})

	t.Run("åˆ†æ®µä¸Šä¼ é”™è¯¯æƒ…å†µ", func(t *testing.T) {
		// åœ¨ä¸å­˜åœ¨çš„æ¡¶ä¸­åˆ›å»ºåˆ†æ®µä¸Šä¼ 
		_, err := s.CreateMultipartUpload("nonexistent-bucket", "key", "text/plain", nil)
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "does not exist")

		// ä½¿ç”¨æ— æ•ˆçš„åˆ†æ®µå·
		uploadID, err := s.CreateMultipartUpload(bucketName, "test-key", "text/plain", nil)
		assert.NoError(t, err)

		_, err = s.UploadPart(bucketName, "test-key", uploadID, 0, []byte("data")) // åˆ†æ®µå·å¿…é¡»>=1
		assert.Error(t, err)

		_, err = s.UploadPart(bucketName, "test-key", uploadID, 10001, []byte("data")) // åˆ†æ®µå·å¿…é¡»<=10000
		assert.Error(t, err)

		// ä½¿ç”¨ä¸å­˜åœ¨çš„ä¸Šä¼ ID
		_, err = s.UploadPart(bucketName, "test-key", "nonexistent-upload-id", 1, []byte("data"))
		assert.Error(t, err)

		// æ¡¶æˆ–é”®ä¸åŒ¹é…
		_, err = s.UploadPart("wrong-bucket", "test-key", uploadID, 1, []byte("data"))
		assert.Error(t, err)

		_, err = s.UploadPart(bucketName, "wrong-key", uploadID, 1, []byte("data"))
		assert.Error(t, err)

		// å®Œæˆæ²¡æœ‰åˆ†æ®µçš„ä¸Šä¼ 
		_, err = s.CompleteMultipartUpload(bucketName, "test-key", uploadID, []types.MultipartPart{})
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "no parts")
	})

	t.Run("åˆ†æ®µä¸Šä¼ è¾¹ç•Œæµ‹è¯•", func(t *testing.T) {
		key := "boundary-test"
		uploadID, err := s.CreateMultipartUpload(bucketName, key, "application/octet-stream", nil)
		assert.NoError(t, err)

		// æµ‹è¯•ç©ºåˆ†æ®µ
		etag, err := s.UploadPart(bucketName, key, uploadID, 1, []byte{})
		assert.NoError(t, err)
		assert.NotEmpty(t, etag)

		// æµ‹è¯•å¤§åˆ†æ®µ
		largeData := make([]byte, 1024*1024) // 1MB
		for i := range largeData {
			largeData[i] = byte(i % 256)
		}
		etag, err = s.UploadPart(bucketName, key, uploadID, 2, largeData)
		assert.NoError(t, err)
		assert.NotEmpty(t, etag)

		// å®Œæˆä¸Šä¼ 
		parts := []types.MultipartPart{
			{PartNumber: 1, ETag: storage.CalculateETag([]byte{})},
			{PartNumber: 2, ETag: storage.CalculateETag(largeData)},
		}

		// éœ€è¦é‡æ–°è·å–æ­£ç¡®çš„ETag
		listedParts, err := s.ListParts(bucketName, key, uploadID)
		assert.NoError(t, err)

		for i, part := range listedParts {
			parts[i].ETag = part.ETag
		}

		finalETag, err := s.CompleteMultipartUpload(bucketName, key, uploadID, parts)
		assert.NoError(t, err)
		assert.NotEmpty(t, finalETag)
	})
}

// æµ‹è¯•å¹¶å‘å®‰å…¨æ€§
func TestConcurrencySafety(t *testing.T) {
	s, cleanup := setupTestStorage(t)
	defer cleanup()

	bucketName := "concurrent-test-bucket"
	createTestBucket(t, s, bucketName)

	t.Run("å¹¶å‘å¯¹è±¡æ“ä½œ", func(t *testing.T) {
		const numGoroutines = 10
		const numOperations = 100

		var wg sync.WaitGroup
		errors := make(chan error, numGoroutines*numOperations)

		// å¹¶å‘å†™å…¥å¯¹è±¡
		for i := 0; i < numGoroutines; i++ {
			wg.Add(1)
			go func(goroutineID int) {
				defer wg.Done()
				for j := 0; j < numOperations; j++ {
					key := fmt.Sprintf("concurrent-obj-%d-%d", goroutineID, j)
					data := []byte(fmt.Sprintf("data-%d-%d", goroutineID, j))

					obj := &types.S3ObjectData{
						Key:          key,
						Data:         data,
						ContentType:  "text/plain",
						LastModified: time.Now(),
						ETag:         storage.CalculateETag(data),
					}

					if err := s.PutObject(bucketName, obj); err != nil {
						errors <- err
						return
					}
				}
			}(i)
		}

		wg.Wait()
		close(errors)

		// æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
		for err := range errors {
			t.Errorf("å¹¶å‘å†™å…¥é”™è¯¯: %v", err)
		}

		// éªŒè¯æ‰€æœ‰å¯¹è±¡éƒ½è¢«æ­£ç¡®å†™å…¥
		objects, _, err := s.ListObjects(bucketName, "concurrent-obj-", "", "", 10000)
		assert.NoError(t, err)
		assert.Len(t, objects, numGoroutines*numOperations)
	})

	t.Run("å¹¶å‘åˆ†æ®µä¸Šä¼ ", func(t *testing.T) {
		const numUploads = 5
		var wg sync.WaitGroup
		errors := make(chan error, numUploads*10)

		for i := 0; i < numUploads; i++ {
			wg.Add(1)
			go func(uploadID int) {
				defer wg.Done()

				key := fmt.Sprintf("concurrent-multipart-%d", uploadID)

				// åˆ›å»ºåˆ†æ®µä¸Šä¼ 
				id, err := s.CreateMultipartUpload(bucketName, key, "text/plain", nil)
				if err != nil {
					errors <- err
					return
				}

				// ä¸Šä¼ å¤šä¸ªåˆ†æ®µ
				parts := []types.MultipartPart{}
				for partNum := 1; partNum <= 3; partNum++ {
					data := []byte(fmt.Sprintf("part-%d-%d", uploadID, partNum))
					etag, err := s.UploadPart(bucketName, key, id, partNum, data)
					if err != nil {
						errors <- err
						return
					}
					parts = append(parts, types.MultipartPart{
						PartNumber: partNum,
						ETag:       etag,
					})
				}

				// å®Œæˆä¸Šä¼ 
				_, err = s.CompleteMultipartUpload(bucketName, key, id, parts)
				if err != nil {
					errors <- err
					return
				}
			}(i)
		}

		wg.Wait()
		close(errors)

		// æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
		for err := range errors {
			t.Errorf("å¹¶å‘åˆ†æ®µä¸Šä¼ é”™è¯¯: %v", err)
		}

		// éªŒè¯æ‰€æœ‰å¯¹è±¡éƒ½è¢«æ­£ç¡®åˆ›å»º
		objects, _, err := s.ListObjects(bucketName, "concurrent-multipart-", "", "", 100)
		assert.NoError(t, err)
		assert.Len(t, objects, numUploads)
	})
}

// æµ‹è¯•é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
func TestErrorHandlingAndRecovery(t *testing.T) {
	s, cleanup := setupTestStorage(t)
	defer cleanup()

	t.Run("æ•°æ®åº“å…³é—­åçš„æ“ä½œ", func(t *testing.T) {
		// åˆ›å»ºä¸€ä¸ªæ–°çš„å­˜å‚¨å®ä¾‹ç”¨äºæµ‹è¯•
		tempDir, err := os.MkdirTemp("", "badger_test_close_*")
		require.NoError(t, err)
		defer os.RemoveAll(tempDir)

		testStorage, err := NewBadgerS3Storage(tempDir)
		require.NoError(t, err)

		badgerStorage := testStorage.(*BadgerS3Storage)

		// å…³é—­æ•°æ®åº“
		err = badgerStorage.Close()
		assert.NoError(t, err)

		// å°è¯•åœ¨å…³é—­åè¿›è¡Œæ“ä½œåº”è¯¥è¿”å›é”™è¯¯
		err = badgerStorage.CreateBucket("test-bucket")
		assert.Error(t, err)
	})

	t.Run("æ— æ•ˆæ•°æ®å¤„ç†", func(t *testing.T) {
		bucketName := "error-test-bucket"
		createTestBucket(t, s, bucketName)

		// è¿™äº›æµ‹è¯•ä¾èµ–äºå†…éƒ¨å®ç°ï¼Œä¸»è¦æµ‹è¯•è¾¹ç•Œæƒ…å†µ
		// åœ¨å®é™…å®ç°ä¸­ï¼ŒsafeItemValueå‡½æ•°ä¼šå¤„ç†ç©ºå€¼ç­‰æƒ…å†µ

		// æµ‹è¯•ç©ºå¯¹è±¡æ•°æ®
		obj := &types.S3ObjectData{
			Key:          "empty-metadata-test",
			Data:         []byte("test"),
			ContentType:  "",
			LastModified: time.Now(),
			ETag:         "",
			Metadata:     nil,
		}

		err := s.PutObject(bucketName, obj)
		assert.NoError(t, err)

		retrieved, err := s.GetObject(bucketName, "empty-metadata-test")
		assert.NoError(t, err)
		assert.NotNil(t, retrieved)
	})

	t.Run("è¿‡æœŸåˆ†æ®µä¸Šä¼ æ¸…ç†", func(t *testing.T) {
		bucketName := "cleanup-test-bucket"
		createTestBucket(t, s, bucketName)

		// è¿™ä¸ªæµ‹è¯•éœ€è¦æ¨¡æ‹Ÿæ—¶é—´æµé€ï¼Œåœ¨å®é™…ç¯å¢ƒä¸­å¯èƒ½éœ€è¦è°ƒæ•´
		// åˆ›å»ºä¸€ä¸ªåˆ†æ®µä¸Šä¼ ä½†ä¸å®Œæˆå®ƒ
		key := "expired-upload"
		uploadID, err := s.CreateMultipartUpload(bucketName, key, "text/plain", nil)
		assert.NoError(t, err)

		// ä¸Šä¼ ä¸€ä¸ªåˆ†æ®µ
		_, err = s.UploadPart(bucketName, key, uploadID, 1, []byte("test-data"))
		assert.NoError(t, err)

		// éªŒè¯åˆ†æ®µä¸Šä¼ å­˜åœ¨
		uploads, err := s.ListMultipartUploads(bucketName)
		assert.NoError(t, err)

		found := false
		for _, upload := range uploads {
			if upload.UploadID == uploadID {
				found = true
				break
			}
		}
		assert.True(t, found, "åˆ†æ®µä¸Šä¼ åº”è¯¥å­˜åœ¨")

		// æ‰‹åŠ¨è§¦å‘æ¸…ç†ï¼ˆåœ¨å®é™…å®ç°ä¸­ï¼Œè¿™ä¼šåœ¨åå°å®šæœŸæ‰§è¡Œï¼‰
		// æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•å¯èƒ½éœ€è¦æ ¹æ®å®é™…çš„æ¸…ç†é€»è¾‘è¿›è¡Œè°ƒæ•´
		err = s.cleanupExpiredUploads()
		assert.NoError(t, err)
	})
}

// æµ‹è¯•é‡è¯•æœºåˆ¶
func TestRetryMechanism(t *testing.T) {
	s, cleanup := setupTestStorage(t)
	defer cleanup()

	t.Run("é‡è¯•æœºåˆ¶æ­£å¸¸å·¥ä½œ", func(t *testing.T) {
		// æµ‹è¯•withRetryå‡½æ•°
		callCount := 0
		err := s.withRetry(func() error {
			callCount++
			if callCount < 2 {
				return fmt.Errorf("temporary error")
			}
			return nil
		})

		assert.NoError(t, err)
		assert.Equal(t, 2, callCount, "åº”è¯¥é‡è¯•ä¸€æ¬¡åæˆåŠŸ")
	})

	t.Run("é‡è¯•æœºåˆ¶è¾¾åˆ°æœ€å¤§æ¬¡æ•°", func(t *testing.T) {
		callCount := 0
		err := s.withRetry(func() error {
			callCount++
			return fmt.Errorf("persistent error")
		})

		assert.Error(t, err)
		assert.Equal(t, maxRetries, callCount, "åº”è¯¥é‡è¯•æœ€å¤§æ¬¡æ•°")
		assert.Contains(t, err.Error(), "operation failed after")
	})
}

// æµ‹è¯•å®‰å…¨å‡½æ•°
func TestSafetyFunctions(t *testing.T) {
	s, cleanup := setupTestStorage(t)
	defer cleanup()

	bucketName := "safety-test-bucket"
	createTestBucket(t, s, bucketName)

	t.Run("safeItemValueå¤„ç†nil item", func(t *testing.T) {
		err := s.safeItemValue(nil, func([]byte) error {
			return nil
		})
		assert.Error(t, err)
		assert.Contains(t, err.Error(), "item is nil")
	})

	t.Run("safeIteratorOperationæ­£å¸¸å·¥ä½œ", func(t *testing.T) {
		// åˆ›å»ºä¸€äº›æµ‹è¯•æ•°æ®
		for i := 0; i < 5; i++ {
			key := fmt.Sprintf("test-key-%d", i)
			createTestObject(t, s, bucketName, key, []byte(fmt.Sprintf("data-%d", i)))
		}

		// ä½¿ç”¨safeIteratorOperationéå†
		count := 0
		err := s.db.View(func(txn *badger.Txn) error {
			return s.safeIteratorOperation(txn, []byte(objectsBucketPrefix+bucketName+":"), func(it *badger.Iterator) error {
				for it.Rewind(); it.Valid(); it.Next() {
					count++
				}
				return nil
			})
		})

		assert.NoError(t, err)
		assert.Equal(t, 5, count, "åº”è¯¥éå†åˆ°5ä¸ªå¯¹è±¡")
	})
}

// æ€§èƒ½æµ‹è¯•
func TestPerformance(t *testing.T) {
	if testing.Short() {
		t.Skip("è·³è¿‡æ€§èƒ½æµ‹è¯•")
	}

	s, cleanup := setupTestStorage(t)
	defer cleanup()

	bucketName := "perf-test-bucket"
	createTestBucket(t, s, bucketName)

	t.Run("å¤§é‡å¯¹è±¡å†™å…¥æ€§èƒ½", func(t *testing.T) {
		const numObjects = 1000
		start := time.Now()

		for i := 0; i < numObjects; i++ {
			key := fmt.Sprintf("perf-obj-%d", i)
			data := []byte(fmt.Sprintf("performance-test-data-%d", i))
			createTestObject(t, s, bucketName, key, data)
		}

		duration := time.Since(start)
		t.Logf("å†™å…¥%dä¸ªå¯¹è±¡è€—æ—¶: %v", numObjects, duration)
		t.Logf("å¹³å‡æ¯ä¸ªå¯¹è±¡: %v", duration/numObjects)

		// éªŒè¯å†™å…¥çš„å¯¹è±¡æ•°é‡
		objects, _, err := s.ListObjects(bucketName, "perf-obj-", "", "", numObjects+100)
		assert.NoError(t, err)
		assert.Len(t, objects, numObjects)
	})

	t.Run("å¤§é‡å¯¹è±¡è¯»å–æ€§èƒ½", func(t *testing.T) {
		const numReads = 500
		start := time.Now()

		for i := 0; i < numReads; i++ {
			key := fmt.Sprintf("perf-obj-%d", i)
			_, err := s.GetObject(bucketName, key)
			assert.NoError(t, err)
		}

		duration := time.Since(start)
		t.Logf("è¯»å–%dä¸ªå¯¹è±¡è€—æ—¶: %v", numReads, duration)
		t.Logf("å¹³å‡æ¯ä¸ªå¯¹è±¡: %v", duration/numReads)
	})

	t.Run("å¤§å¯¹è±¡å¤„ç†æ€§èƒ½", func(t *testing.T) {
		// æµ‹è¯•10MBå¯¹è±¡
		largeData := make([]byte, 10*1024*1024)
		for i := range largeData {
			largeData[i] = byte(i % 256)
		}

		key := "large-perf-object"
		start := time.Now()
		createTestObject(t, s, bucketName, key, largeData)
		writeTime := time.Since(start)

		start = time.Now()
		retrieved, err := s.GetObject(bucketName, key)
		readTime := time.Since(start)

		assert.NoError(t, err)
		assert.Equal(t, largeData, retrieved.Data)

		t.Logf("å†™å…¥10MBå¯¹è±¡è€—æ—¶: %v", writeTime)
		t.Logf("è¯»å–10MBå¯¹è±¡è€—æ—¶: %v", readTime)
	})
}

// å‹åŠ›æµ‹è¯•
func TestStressTest(t *testing.T) {
	if testing.Short() {
		t.Skip("è·³è¿‡å‹åŠ›æµ‹è¯•")
	}

	s, cleanup := setupTestStorage(t)
	defer cleanup()

	bucketName := "stress-test-bucket"
	createTestBucket(t, s, bucketName)

	t.Run("é«˜å¹¶å‘è¯»å†™å‹åŠ›æµ‹è¯•", func(t *testing.T) {
		const numGoroutines = 20
		const numOperationsPerGoroutine = 50

		var wg sync.WaitGroup
		errors := make(chan error, numGoroutines*numOperationsPerGoroutine*2)

		// å¯åŠ¨å¤šä¸ªgoroutineè¿›è¡Œå¹¶å‘è¯»å†™
		for i := 0; i < numGoroutines; i++ {
			wg.Add(1)
			go func(goroutineID int) {
				defer wg.Done()

				for j := 0; j < numOperationsPerGoroutine; j++ {
					key := fmt.Sprintf("stress-obj-%d-%d", goroutineID, j)
					data := make([]byte, 1024) // 1KBæ•°æ®
					for k := range data {
						data[k] = byte((goroutineID + j + k) % 256)
					}

					// å†™å…¥å¯¹è±¡
					obj := &types.S3ObjectData{
						Key:          key,
						Data:         data,
						ContentType:  "application/octet-stream",
						LastModified: time.Now(),
						ETag:         storage.CalculateETag(data),
					}

					if err := s.PutObject(bucketName, obj); err != nil {
						errors <- fmt.Errorf("å†™å…¥é”™è¯¯ %s: %v", key, err)
						continue
					}

					// ç«‹å³è¯»å–éªŒè¯
					retrieved, err := s.GetObject(bucketName, key)
					if err != nil {
						errors <- fmt.Errorf("è¯»å–é”™è¯¯ %s: %v", key, err)
						continue
					}

					if !bytes.Equal(data, retrieved.Data) {
						errors <- fmt.Errorf("æ•°æ®ä¸åŒ¹é… %s", key)
					}
				}
			}(i)
		}

		wg.Wait()
		close(errors)

		// æ”¶é›†æ‰€æœ‰é”™è¯¯
		var allErrors []error
		for err := range errors {
			allErrors = append(allErrors, err)
		}

		if len(allErrors) > 0 {
			t.Errorf("å‹åŠ›æµ‹è¯•ä¸­å‘ç° %d ä¸ªé”™è¯¯:", len(allErrors))
			for i, err := range allErrors {
				if i < 10 { // åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
					t.Errorf("  %v", err)
				}
			}
			if len(allErrors) > 10 {
				t.Errorf("  ... è¿˜æœ‰ %d ä¸ªé”™è¯¯", len(allErrors)-10)
			}
		}

		// éªŒè¯æœ€ç»ˆå¯¹è±¡æ•°é‡
		objects, _, err := s.ListObjects(bucketName, "stress-obj-", "", "", 10000)
		assert.NoError(t, err)
		expectedCount := numGoroutines * numOperationsPerGoroutine
		assert.Len(t, objects, expectedCount, "å¯¹è±¡æ•°é‡åº”è¯¥åŒ¹é…")
	})

	t.Run("åˆ†æ®µä¸Šä¼ å‹åŠ›æµ‹è¯•", func(t *testing.T) {
		const numUploads = 10
		const partsPerUpload = 5

		var wg sync.WaitGroup
		errors := make(chan error, numUploads*10)

		for i := 0; i < numUploads; i++ {
			wg.Add(1)
			go func(uploadIndex int) {
				defer wg.Done()

				key := fmt.Sprintf("stress-multipart-%d", uploadIndex)

				// åˆ›å»ºåˆ†æ®µä¸Šä¼ 
				uploadID, err := s.CreateMultipartUpload(bucketName, key, "application/octet-stream", nil)
				if err != nil {
					errors <- fmt.Errorf("åˆ›å»ºåˆ†æ®µä¸Šä¼ å¤±è´¥ %s: %v", key, err)
					return
				}

				// ä¸Šä¼ å¤šä¸ªåˆ†æ®µ
				parts := make([]types.MultipartPart, partsPerUpload)
				for partNum := 1; partNum <= partsPerUpload; partNum++ {
					data := make([]byte, 1024*10) // 10KB per part
					for j := range data {
						data[j] = byte((uploadIndex + partNum + j) % 256)
					}

					etag, err := s.UploadPart(bucketName, key, uploadID, partNum, data)
					if err != nil {
						errors <- fmt.Errorf("ä¸Šä¼ åˆ†æ®µå¤±è´¥ %s part %d: %v", key, partNum, err)
						return
					}

					parts[partNum-1] = types.MultipartPart{
						PartNumber: partNum,
						ETag:       etag,
					}
				}

				// å®Œæˆä¸Šä¼ 
				_, err = s.CompleteMultipartUpload(bucketName, key, uploadID, parts)
				if err != nil {
					errors <- fmt.Errorf("å®Œæˆåˆ†æ®µä¸Šä¼ å¤±è´¥ %s: %v", key, err)
					return
				}

				// éªŒè¯æœ€ç»ˆå¯¹è±¡
				obj, err := s.GetObject(bucketName, key)
				if err != nil {
					errors <- fmt.Errorf("è·å–æœ€ç»ˆå¯¹è±¡å¤±è´¥ %s: %v", key, err)
					return
				}

				expectedSize := partsPerUpload * 1024 * 10
				if len(obj.Data) != expectedSize {
					errors <- fmt.Errorf("å¯¹è±¡å¤§å°ä¸åŒ¹é… %s: expected %d, got %d", key, expectedSize, len(obj.Data))
				}
			}(i)
		}

		wg.Wait()
		close(errors)

		// æ”¶é›†é”™è¯¯
		var allErrors []error
		for err := range errors {
			allErrors = append(allErrors, err)
		}

		if len(allErrors) > 0 {
			t.Errorf("åˆ†æ®µä¸Šä¼ å‹åŠ›æµ‹è¯•ä¸­å‘ç° %d ä¸ªé”™è¯¯:", len(allErrors))
			for _, err := range allErrors {
				t.Errorf("  %v", err)
			}
		}
	})
}

// è¾¹ç•Œæ¡ä»¶æµ‹è¯•
func TestBoundaryConditions(t *testing.T) {
	s, cleanup := setupTestStorage(t)
	defer cleanup()

	bucketName := "boundary-test-bucket"
	createTestBucket(t, s, bucketName)

	t.Run("æé™å¤§å°æµ‹è¯•", func(t *testing.T) {
		// æµ‹è¯•æ¥è¿‘å†…å­˜é™åˆ¶çš„å¯¹è±¡ï¼ˆæ ¹æ®ç³»ç»Ÿè°ƒæ•´ï¼‰
		if testing.Short() {
			t.Skip("è·³è¿‡å¤§å¯¹è±¡æµ‹è¯•")
		}

		// 50MBå¯¹è±¡
		largeSize := 50 * 1024 * 1024
		largeData := make([]byte, largeSize)
		for i := range largeData {
			largeData[i] = byte(i % 256)
		}

		key := "extreme-large-object"
		start := time.Now()
		createTestObject(t, s, bucketName, key, largeData)
		t.Logf("å†™å…¥50MBå¯¹è±¡è€—æ—¶: %v", time.Since(start))

		start = time.Now()
		retrieved, err := s.GetObject(bucketName, key)
		assert.NoError(t, err)
		t.Logf("è¯»å–50MBå¯¹è±¡è€—æ—¶: %v", time.Since(start))

		assert.Equal(t, largeData, retrieved.Data)
	})

	t.Run("æé™æ•°é‡æµ‹è¯•", func(t *testing.T) {
		if testing.Short() {
			t.Skip("è·³è¿‡å¤§é‡å¯¹è±¡æµ‹è¯•")
		}

		// åˆ›å»ºå¤§é‡å°å¯¹è±¡
		const numObjects = 10000
		start := time.Now()

		for i := 0; i < numObjects; i++ {
			key := fmt.Sprintf("tiny-obj-%06d", i)
			data := []byte(fmt.Sprintf("%d", i))
			createTestObject(t, s, bucketName, key, data)

			if i%1000 == 0 {
				t.Logf("å·²åˆ›å»º %d ä¸ªå¯¹è±¡", i)
			}
		}

		t.Logf("åˆ›å»º%dä¸ªå¯¹è±¡æ€»è€—æ—¶: %v", numObjects, time.Since(start))

		// éªŒè¯åˆ—å‡ºå¯¹è±¡çš„æ€§èƒ½
		start = time.Now()
		objects, _, err := s.ListObjects(bucketName, "tiny-obj-", "", "", numObjects+1000)
		assert.NoError(t, err)
		t.Logf("åˆ—å‡º%dä¸ªå¯¹è±¡è€—æ—¶: %v", len(objects), time.Since(start))

		assert.GreaterOrEqual(t, len(objects), numObjects)
	})

	t.Run("Unicodeå’Œç‰¹æ®Šå­—ç¬¦æµ‹è¯•", func(t *testing.T) {
		testCases := []struct {
			name string
			key  string
		}{
			{"ä¸­æ–‡é”®", "æµ‹è¯•å¯¹è±¡é”®"},
			{"æ—¥æ–‡é”®", "ãƒ†ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼"},
			{"éŸ©æ–‡é”®", "í…ŒìŠ¤íŠ¸ê°ì²´í‚¤"},
			{"è¡¨æƒ…ç¬¦å·", "test-ğŸš€-object-ğŸ‰"},
			{"ç‰¹æ®Šç¬¦å·", "test!@#$%^&*()_+-=[]{}|;':\",./<>?"},
			{"ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦", "test object\twith\nspecial\rchars"},
			{"é•¿Unicode", "è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„ä¸­æ–‡å¯¹è±¡é”®åç§°ç”¨æ¥æµ‹è¯•Unicodeå­—ç¬¦çš„å¤„ç†èƒ½åŠ›"},
		}

		for _, tc := range testCases {
			t.Run(tc.name, func(t *testing.T) {
				data := []byte("Unicode test data: " + tc.key)
				createTestObject(t, s, bucketName, tc.key, data)

				retrieved, err := s.GetObject(bucketName, tc.key)
				assert.NoError(t, err)
				assert.Equal(t, data, retrieved.Data)
				assert.Equal(t, tc.key, retrieved.Key)
			})
		}
	})
}

// æ•°æ®ä¸€è‡´æ€§æµ‹è¯•
func TestDataConsistency(t *testing.T) {
	s, cleanup := setupTestStorage(t)
	defer cleanup()

	bucketName := "consistency-test-bucket"
	createTestBucket(t, s, bucketName)

	t.Run("å†™å…¥åç«‹å³è¯»å–ä¸€è‡´æ€§", func(t *testing.T) {
		const numTests = 100

		for i := 0; i < numTests; i++ {
			key := fmt.Sprintf("consistency-test-%d", i)
			originalData := make([]byte, 1024)
			for j := range originalData {
				originalData[j] = byte((i + j) % 256)
			}

			// å†™å…¥
			createTestObject(t, s, bucketName, key, originalData)

			// ç«‹å³è¯»å–
			retrieved, err := s.GetObject(bucketName, key)
			assert.NoError(t, err)
			assert.Equal(t, originalData, retrieved.Data, "ç¬¬%dæ¬¡æµ‹è¯•æ•°æ®ä¸ä¸€è‡´", i)
		}
	})

	t.Run("è¦†ç›–å†™å…¥ä¸€è‡´æ€§", func(t *testing.T) {
		key := "overwrite-test"

		// ç¬¬ä¸€æ¬¡å†™å…¥
		data1 := []byte("first version of data")
		createTestObject(t, s, bucketName, key, data1)

		retrieved, err := s.GetObject(bucketName, key)
		assert.NoError(t, err)
		assert.Equal(t, data1, retrieved.Data)

		// è¦†ç›–å†™å…¥
		data2 := []byte("second version of data - much longer than the first version")
		createTestObject(t, s, bucketName, key, data2)

		retrieved, err = s.GetObject(bucketName, key)
		assert.NoError(t, err)
		assert.Equal(t, data2, retrieved.Data)
		assert.NotEqual(t, data1, retrieved.Data)
	})

	t.Run("åˆ†æ®µä¸Šä¼ æ•°æ®ä¸€è‡´æ€§", func(t *testing.T) {
		key := "multipart-consistency-test"

		// å‡†å¤‡æµ‹è¯•æ•°æ®
		partData := [][]byte{
			make([]byte, 1024),
			make([]byte, 2048),
			make([]byte, 512),
		}

		for i, data := range partData {
			for j := range data {
				data[j] = byte((i*1000 + j) % 256)
			}
		}

		// åˆ›å»ºåˆ†æ®µä¸Šä¼ 
		uploadID, err := s.CreateMultipartUpload(bucketName, key, "application/octet-stream", nil)
		assert.NoError(t, err)

		// ä¸Šä¼ åˆ†æ®µ
		parts := make([]types.MultipartPart, len(partData))
		for i, data := range partData {
			etag, err := s.UploadPart(bucketName, key, uploadID, i+1, data)
			assert.NoError(t, err)
			parts[i] = types.MultipartPart{
				PartNumber: i + 1,
				ETag:       etag,
			}
		}

		// å®Œæˆä¸Šä¼ 
		_, err = s.CompleteMultipartUpload(bucketName, key, uploadID, parts)
		assert.NoError(t, err)

		// éªŒè¯æœ€ç»ˆæ•°æ®
		retrieved, err := s.GetObject(bucketName, key)
		assert.NoError(t, err)

		expectedData := bytes.Join(partData, nil)
		assert.Equal(t, expectedData, retrieved.Data)
	})
}

// Benchmarkæµ‹è¯•
func BenchmarkBadgerStorage(b *testing.B) {
	// åˆ›å»ºæµ‹è¯•å­˜å‚¨ï¼Œä½†ä¸ä½¿ç”¨testing.T
	tempDir, err := os.MkdirTemp("", "badger_bench_")
	if err != nil {
		b.Fatal(err)
	}
	defer os.RemoveAll(tempDir)

	s3Storage, err := NewBadgerS3Storage(tempDir)
	if err != nil {
		b.Fatal(err)
	}
	defer s3Storage.Close()

	s := s3Storage.(*BadgerS3Storage)
	bucketName := "benchmark-bucket"
	s.CreateBucket(bucketName)

	b.Run("PutObject", func(b *testing.B) {
		data := make([]byte, 1024) // 1KB
		for i := range data {
			data[i] = byte(i % 256)
		}

		b.ResetTimer()
		for i := 0; i < b.N; i++ {
			key := fmt.Sprintf("bench-obj-%d", i)
			obj := &types.S3ObjectData{
				Key:          key,
				Data:         data,
				ContentType:  "application/octet-stream",
				LastModified: time.Now(),
				ETag:         storage.CalculateETag(data),
			}
			s.PutObject(bucketName, obj)
		}
	})

	b.Run("GetObject", func(b *testing.B) {
		// é¢„å…ˆåˆ›å»ºå¯¹è±¡
		data := make([]byte, 1024)
		for i := 0; i < 1000; i++ {
			key := fmt.Sprintf("get-bench-obj-%d", i)
			obj := &types.S3ObjectData{
				Key:          key,
				Data:         data,
				ContentType:  "application/octet-stream",
				LastModified: time.Now(),
				ETag:         storage.CalculateETag(data),
			}
			s.PutObject(bucketName, obj)
		}

		b.ResetTimer()
		for i := 0; i < b.N; i++ {
			key := fmt.Sprintf("get-bench-obj-%d", i%1000)
			s.GetObject(bucketName, key)
		}
	})

	b.Run("ListObjects", func(b *testing.B) {
		b.ResetTimer()
		for i := 0; i < b.N; i++ {
			s.ListObjects(bucketName, "", "", "", 100)
		}
	})
}

/*
import (
	"bytes"
	"encoding/json"
	"fmt"
	"io/ioutil"
	"os"
	"testing"
	"time"

	"github.com/dgraph-io/badger/v3"
	"github.com/elastic-io/haven/internal/log"
	"github.com/elastic-io/haven/internal/storage"
	"github.com/elastic-io/haven/internal/types"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

// åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
func setupTestDB(t *testing.T) (*BadgerS3Storage, string) {
	tempDir, err := ioutil.TempDir("", "badger-test-*")
	require.NoError(t, err)

	log.Init("", "debug")

	storage, err := NewBadgerS3Storage(tempDir)
	require.NoError(t, err)

	s, ok := storage.(*BadgerS3Storage)
	require.True(t, ok)

	return s, tempDir
}

// æ¸…ç†æµ‹è¯•èµ„æº
func teardownTestDB(s *BadgerS3Storage, tempDir string) {
	s.Close()
	os.RemoveAll(tempDir)
}

// æµ‹è¯•å­˜å‚¨æ¡¶æ“ä½œ
func TestBucketOperations(t *testing.T) {
	s, tempDir := setupTestDB(t)
	defer teardownTestDB(s, tempDir)

	// æµ‹è¯•åˆ›å»ºå­˜å‚¨æ¡¶
	err := s.CreateBucket("testbucket")
	assert.NoError(t, err)

	// æµ‹è¯•å­˜å‚¨æ¡¶å­˜åœ¨
	exists, err := s.BucketExists("testbucket")
	assert.NoError(t, err)
	assert.True(t, exists)

	// æµ‹è¯•ä¸å­˜åœ¨çš„å­˜å‚¨æ¡¶
	exists, err = s.BucketExists("nonexistentbucket")
	assert.NoError(t, err)
	assert.False(t, exists)

	// æµ‹è¯•åˆ—å‡ºå­˜å‚¨æ¡¶
	buckets, err := s.ListBuckets()
	assert.NoError(t, err)
	assert.Len(t, buckets, 1)
	assert.Equal(t, "testbucket", buckets[0].Name)

	// æµ‹è¯•åˆ é™¤å­˜å‚¨æ¡¶
	err = s.DeleteBucket("testbucket")
	assert.NoError(t, err)

	// éªŒè¯å­˜å‚¨æ¡¶å·²åˆ é™¤
	exists, err = s.BucketExists("testbucket")
	assert.NoError(t, err)
	assert.False(t, exists)
}

// æµ‹è¯•å¯¹è±¡æ“ä½œ
func TestObjectOperations(t *testing.T) {
	s, tempDir := setupTestDB(t)
	defer teardownTestDB(s, tempDir)

	// åˆ›å»ºæµ‹è¯•å­˜å‚¨æ¡¶
	err := s.CreateBucket("testbucket")
	require.NoError(t, err)

	// æµ‹è¯•ä¸Šä¼ å¯¹è±¡
	testData := []byte("Hello, World!")
	testObject := &types.S3ObjectData{
		Key:          "testobject.txt",
		Data:         testData,
		ContentType:  "text/plain",
		LastModified: time.Now(),
		ETag:         storage.CalculateETag(testData),
		Metadata:     map[string]string{"test": "value"},
	}

	err = s.PutObject("testbucket", testObject)
	assert.NoError(t, err)

	// æµ‹è¯•è·å–å¯¹è±¡
	retrievedObject, err := s.GetObject("testbucket", "testobject.txt")
	assert.NoError(t, err)
	assert.Equal(t, testObject.Key, retrievedObject.Key)
	assert.Equal(t, testObject.ContentType, retrievedObject.ContentType)
	assert.Equal(t, testObject.ETag, retrievedObject.ETag)
	assert.Equal(t, testObject.Metadata["test"], retrievedObject.Metadata["test"])
	assert.True(t, bytes.Equal(testObject.Data, retrievedObject.Data))

	// æµ‹è¯•åˆ—å‡ºå¯¹è±¡
	objects, prefixes, err := s.ListObjects("testbucket", "", "", "", 1000)
	assert.NoError(t, err)
	assert.Len(t, objects, 1)
	assert.Len(t, prefixes, 0)
	assert.Equal(t, "testobject.txt", objects[0].Key)

	// æµ‹è¯•å¸¦å‰ç¼€çš„åˆ—å‡ºå¯¹è±¡
	objects, prefixes, err = s.ListObjects("testbucket", "test", "", "", 1000)
	assert.NoError(t, err)
	assert.Len(t, objects, 1)
	assert.Len(t, prefixes, 0)

	// æµ‹è¯•å¸¦åˆ†éš”ç¬¦çš„åˆ—å‡ºå¯¹è±¡
	// å…ˆæ·»åŠ ä¸€ä¸ªå¸¦è·¯å¾„çš„å¯¹è±¡
	pathObject := &types.S3ObjectData{
		Key:          "folder/nested.txt",
		Data:         []byte("Nested content"),
		ContentType:  "text/plain",
		LastModified: time.Now(),
		ETag:         storage.CalculateETag([]byte("Nested content")),
	}
	err = s.PutObject("testbucket", pathObject)
	assert.NoError(t, err)

	objects, prefixes, err = s.ListObjects("testbucket", "", "", "/", 1000)
	assert.NoError(t, err)
	assert.Len(t, objects, 1)  // åªæœ‰æ ¹ç›®å½•çš„å¯¹è±¡
	assert.Len(t, prefixes, 1) // ä¸€ä¸ªå‰ç¼€ "folder/"
	assert.Equal(t, "folder/", prefixes[0])

	// æµ‹è¯•åˆ é™¤å¯¹è±¡
	err = s.DeleteObject("testbucket", "testobject.txt")
	assert.NoError(t, err)

	// éªŒè¯å¯¹è±¡å·²åˆ é™¤
	_, err = s.GetObject("testbucket", "testobject.txt")
	assert.Error(t, err)

	// æµ‹è¯•åˆ é™¤ä¸å­˜åœ¨çš„å¯¹è±¡
	err = s.DeleteObject("testbucket", "nonexistent.txt")
	assert.Error(t, err)

	// æµ‹è¯•æ— æ³•åˆ é™¤éç©ºå­˜å‚¨æ¡¶
	err = s.DeleteBucket("testbucket")
	assert.Error(t, err)

	// åˆ é™¤å‰©ä½™å¯¹è±¡ååº”è¯¥å¯ä»¥åˆ é™¤å­˜å‚¨æ¡¶
	err = s.DeleteObject("testbucket", "folder/nested.txt")
	assert.NoError(t, err)
	err = s.DeleteBucket("testbucket")
	assert.NoError(t, err)
}

// æµ‹è¯•åˆ†æ®µä¸Šä¼ 
func TestMultipartUpload(t *testing.T) {
	s, tempDir := setupTestDB(t)
	defer teardownTestDB(s, tempDir)

	// åˆ›å»ºæµ‹è¯•å­˜å‚¨æ¡¶
	err := s.CreateBucket("testbucket")
	require.NoError(t, err)

	// åˆå§‹åŒ–åˆ†æ®µä¸Šä¼ 
	uploadID, err := s.CreateMultipartUpload("testbucket", "multipart.txt", "text/plain", map[string]string{"test": "value"})
	assert.NoError(t, err)
	assert.NotEmpty(t, uploadID)

	// ä¸Šä¼ åˆ†æ®µ
	part1Data := []byte("This is part 1 ")
	part2Data := []byte("and this is part 2")

	etag1, err := s.UploadPart("testbucket", "multipart.txt", uploadID, 1, part1Data)
	assert.NoError(t, err)
	assert.NotEmpty(t, etag1)

	etag2, err := s.UploadPart("testbucket", "multipart.txt", uploadID, 2, part2Data)
	assert.NoError(t, err)
	assert.NotEmpty(t, etag2)

	// åˆ—å‡ºåˆ†æ®µ
	parts, err := s.ListParts("testbucket", "multipart.txt", uploadID)
	assert.NoError(t, err)
	assert.Len(t, parts, 2)
	assert.Equal(t, 1, parts[0].PartNumber)
	assert.Equal(t, 2, parts[1].PartNumber)

	// åˆ—å‡ºè¿›è¡Œä¸­çš„ä¸Šä¼ 
	uploads, err := s.ListMultipartUploads("testbucket")
	assert.NoError(t, err)
	assert.Len(t, uploads, 1)
	assert.Equal(t, "multipart.txt", uploads[0].Key)

	// å®Œæˆåˆ†æ®µä¸Šä¼ 
	completeParts := []types.MultipartPart{
		{PartNumber: 1, ETag: etag1},
		{PartNumber: 2, ETag: etag2},
	}

	finalETag, err := s.CompleteMultipartUpload("testbucket", "multipart.txt", uploadID, completeParts)
	assert.NoError(t, err)
	assert.NotEmpty(t, finalETag)

	// éªŒè¯åˆå¹¶åçš„å¯¹è±¡
	obj, err := s.GetObject("testbucket", "multipart.txt")
	assert.NoError(t, err)
	assert.Equal(t, "multipart.txt", obj.Key)
	assert.Equal(t, "text/plain", obj.ContentType)
	assert.Equal(t, "value", obj.Metadata["test"])

	expectedData := append(part1Data, part2Data...)
	assert.True(t, bytes.Equal(expectedData, obj.Data))

	// éªŒè¯ä¸Šä¼ å·²å®Œæˆï¼ˆä¸å†åœ¨è¿›è¡Œä¸­åˆ—è¡¨ä¸­ï¼‰
	uploads, err = s.ListMultipartUploads("testbucket")
	assert.NoError(t, err)
	assert.Len(t, uploads, 0)

	// æµ‹è¯•ä¸­æ­¢ä¸Šä¼ 
	uploadID2, err := s.CreateMultipartUpload("testbucket", "aborted.txt", "text/plain", nil)
	assert.NoError(t, err)

	_, err = s.UploadPart("testbucket", "aborted.txt", uploadID2, 1, []byte("This will be aborted"))
	assert.NoError(t, err)

	err = s.AbortMultipartUpload("testbucket", "aborted.txt", uploadID2)
	assert.NoError(t, err)

	// éªŒè¯ä¸Šä¼ å·²ä¸­æ­¢
	uploads, err = s.ListMultipartUploads("testbucket")
	assert.NoError(t, err)
	assert.Len(t, uploads, 0)

	// éªŒè¯æ²¡æœ‰åˆ›å»ºå¯¹è±¡
	_, err = s.GetObject("testbucket", "aborted.txt")
	assert.Error(t, err)
}

// TestExpiredUploadCleanup æµ‹è¯•è¿‡æœŸä¸Šä¼ æ¸…ç†
func TestExpiredUploadCleanup(t *testing.T) {
	t.Skip("skip TestExpiredUploadCleanup")
	s, tempDir := setupTestDB(t)
	defer teardownTestDB(s, tempDir)

	// åˆ›å»ºæµ‹è¯•å­˜å‚¨æ¡¶
	err := s.CreateBucket("testbucket")
	require.NoError(t, err)

	// åˆå§‹åŒ–åˆ†æ®µä¸Šä¼ 
	uploadID, err := s.CreateMultipartUpload("testbucket", "expired.txt", "text/plain", nil)
	assert.NoError(t, err)

	// ä¸Šä¼ ä¸€ä¸ªåˆ†æ®µ
	_, err = s.UploadPart("testbucket", "expired.txt", uploadID, 1, []byte("Test data"))
	assert.NoError(t, err)

	// è·å–å½“å‰ä¸Šä¼ ä¿¡æ¯
	var uploadInfo types.MultipartUploadInfo
	err = s.db.View(func(txn *badger.Txn) error {
		item, err := txn.Get([]byte(multipartBucketPrefix + uploadID))
		if err != nil {
			return err
		}

		return item.Value(func(val []byte) error {
			return json.Unmarshal(val, &uploadInfo)
		})
	})
	assert.NoError(t, err)

	// åˆ›å»ºä¸€ä¸ªæ–°çš„è¿‡æœŸä¸Šä¼ ä¿¡æ¯
	uploadInfo.CreatedAt = time.Now().Add(-8 * 24 * time.Hour)
	expiredData, err := json.Marshal(uploadInfo)
	assert.NoError(t, err)

	// æ›´æ–°ä¸Šä¼ ä¿¡æ¯ä½¿å…¶è¿‡æœŸ
	err = s.db.Update(func(txn *badger.Txn) error {
		return txn.Set([]byte(multipartBucketPrefix+uploadID), expiredData)
	})
	assert.NoError(t, err)

	// æ‰‹åŠ¨è§¦å‘æ¸…ç†
	err = s.cleanupExpiredUploads()
	assert.NoError(t, err)

	// éªŒè¯ä¸Šä¼ å·²è¢«æ¸…ç†
	uploads, err := s.ListMultipartUploads("testbucket")
	assert.NoError(t, err)
	assert.Len(t, uploads, 0)
}

// æµ‹è¯•è¾¹ç¼˜æƒ…å†µ
func TestEdgeCases(t *testing.T) {
	s, tempDir := setupTestDB(t)
	defer teardownTestDB(s, tempDir)

	// åˆ›å»ºæµ‹è¯•å­˜å‚¨æ¡¶
	err := s.CreateBucket("testbucket")
	require.NoError(t, err)

	// æµ‹è¯•åˆ›å»ºå·²å­˜åœ¨çš„å­˜å‚¨æ¡¶
	err = s.CreateBucket("testbucket")
	assert.Error(t, err)

	// æµ‹è¯•åˆ é™¤ä¸å­˜åœ¨çš„å­˜å‚¨æ¡¶
	err = s.DeleteBucket("nonexistent")
	assert.Error(t, err)

	// æµ‹è¯•åœ¨ä¸å­˜åœ¨çš„å­˜å‚¨æ¡¶ä¸­æ“ä½œ
	err = s.PutObject("nonexistent", &types.S3ObjectData{Key: "test.txt", Data: []byte("test")})
	assert.Error(t, err)

	_, err = s.GetObject("nonexistent", "test.txt")
	assert.Error(t, err)

	// æµ‹è¯•ç©ºå¯¹è±¡
	emptyObject := &types.S3ObjectData{
		Key:          "empty.txt",
		Data:         []byte{},
		ContentType:  "text/plain",
		LastModified: time.Now(),
		ETag:         storage.CalculateETag([]byte{}),
	}
	err = s.PutObject("testbucket", emptyObject)
	assert.NoError(t, err)

	retrievedEmpty, err := s.GetObject("testbucket", "empty.txt")
	assert.NoError(t, err)
	assert.Equal(t, 0, len(retrievedEmpty.Data))

	// æµ‹è¯•å¤§å¯¹è±¡
	largeData := make([]byte, 5*1024*1024) // 5MB
	for i := range largeData {
		largeData[i] = byte(i % 256)
	}

	largeObject := &types.S3ObjectData{
		Key:          "large.bin",
		Data:         largeData,
		ContentType:  "application/octet-stream",
		LastModified: time.Now(),
		ETag:         storage.CalculateETag(largeData),
	}
	err = s.PutObject("testbucket", largeObject)
	assert.NoError(t, err)

	retrievedLarge, err := s.GetObject("testbucket", "large.bin")
	assert.NoError(t, err)
	assert.Equal(t, len(largeData), len(retrievedLarge.Data))
	assert.True(t, bytes.Equal(largeData, retrievedLarge.Data))

	// æµ‹è¯•åˆ†æ®µä¸Šä¼ çš„è¾¹ç¼˜æƒ…å†µ
	// æ— æ•ˆçš„åˆ†æ®µå·
	_, err = s.UploadPart("testbucket", "test.txt", "invalid-id", 0, []byte("test"))
	assert.Error(t, err)

	_, err = s.UploadPart("testbucket", "test.txt", "invalid-id", 10001, []byte("test"))
	assert.Error(t, err)

	// å®Œæˆä¸Šä¼ æ—¶æ²¡æœ‰æŒ‡å®šåˆ†æ®µ
	uploadID, err := s.CreateMultipartUpload("testbucket", "noparts.txt", "text/plain", nil)
	assert.NoError(t, err)

	_, err = s.CompleteMultipartUpload("testbucket", "noparts.txt", uploadID, []types.MultipartPart{})
	assert.Error(t, err)
}

// æµ‹è¯•å¹¶å‘æ“ä½œ
func TestConcurrentOperations(t *testing.T) {
	s, tempDir := setupTestDB(t)
	defer teardownTestDB(s, tempDir)

	// åˆ›å»ºæµ‹è¯•å­˜å‚¨æ¡¶
	err := s.CreateBucket("testbucket")
	require.NoError(t, err)

	// å¹¶å‘ä¸Šä¼ å¤šä¸ªå¯¹è±¡
	const numObjects = 100
	done := make(chan bool, numObjects)

	for i := 0; i < numObjects; i++ {
		go func(idx int) {
			key := fmt.Sprintf("concurrent-%d.txt", idx)
			data := []byte(fmt.Sprintf("Data for object %d", idx))

			obj := &types.S3ObjectData{
				Key:          key,
				Data:         data,
				ContentType:  "text/plain",
				LastModified: time.Now(),
				ETag:         storage.CalculateETag(data),
			}

			err := s.PutObject("testbucket", obj)
			assert.NoError(t, err)

			done <- true
		}(i)
	}

	// ç­‰å¾…æ‰€æœ‰ä¸Šä¼ å®Œæˆ
	for i := 0; i < numObjects; i++ {
		<-done
	}

	// éªŒè¯æ‰€æœ‰å¯¹è±¡éƒ½å·²ä¸Šä¼ 
	objects, _, err := s.ListObjects("testbucket", "concurrent-", "", "", 1000)
	assert.NoError(t, err)
	assert.Len(t, objects, numObjects)

	// å¹¶å‘è·å–æ‰€æœ‰å¯¹è±¡
	for i := 0; i < numObjects; i++ {
		go func(idx int) {
			key := fmt.Sprintf("concurrent-%d.txt", idx)
			obj, err := s.GetObject("testbucket", key)
			assert.NoError(t, err)
			assert.Equal(t, key, obj.Key)
			assert.Equal(t, fmt.Sprintf("Data for object %d", idx), string(obj.Data))

			done <- true
		}(i)
	}

	// ç­‰å¾…æ‰€æœ‰è·å–å®Œæˆ
	for i := 0; i < numObjects; i++ {
		<-done
	}
}
*/
